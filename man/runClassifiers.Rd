% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fun_runClassifiers.R
\name{runClassifiers}
\alias{runClassifiers}
\title{runClassifiers}
\usage{
runClassifiers(
  preProcess.obj = NULL,
  ...,
  models = c("bag_mlp", "rand_forest", "svm_poly"),
  selector.recipes = c("boruta", "roc", "boruta"),
  tuning.method = "tune_grid",
  n = 5,
  v = 3,
  boruta.maxRuns = 100,
  selector.threshold = 0.95,
  metric = "accuracy",
  nsim = 2,
  filter = TRUE,
  seed = 123,
  downsample = FALSE,
  plot = TRUE,
  parallel = FALSE
)
}
\arguments{
\item{preProcess.obj}{An object of class preProcess.}

\item{...}{Arguments passed to the function. If preProcess.obj is not provided, the function looks
for df.count, df.clin and class in ... as specified in the documentation of preProcess function.}

\item{models}{A character vector specifying the classifiers to be used. Supported models include:

  - `"xgboost"`: Extreme Gradient Boosting, an ensemble method using boosted trees.

  - `"bag_tree"`: Bagged decision trees, a bootstrapped ensemble of tree models.

  - `"lightGBM"`: A fast gradient boosting method optimized for large datasets.

  - `"pls"`: Partial Least Squares regression.

  - `"logistic"`: Logistic regression, a simple linear classifier.

  - `"C5_rules"`: Rule-based classifier using C5.0 decision trees.
  - `"mars"`: Multivariate Adaptive Regression Splines, a flexible non-linear regression method.

  - `"bag_mars"`: Bagged version of MARS for increased stability.

  - `"mlp"`: Multi-Layer Perceptron, a basic feedforward neural network.

  - `"bag_mlp"`: Bagged version of MLP to reduce variance.

  - `"decision_tree"`: A single decision tree.

  - `"rand_forest"`: Random forest, an ensemble of decision trees with randomized splits.

  - `"svm_linear"`: Support Vector Machine with a linear kernel.

  - `"svm_poly"`: Support Vector Machine with a polynomial kernel.

  - `"svm_rbf"`: Support Vector Machine with a radial basis function (RBF) kernel.}

\item{selector.recipes}{A character vector specifying the feature selection methods to be applied before classification.

Supported selection strategies include:

  - `"base"`: Uses only the default pre-processing steps (normalization, near-zero variance removal), with no feature selection.

  - `"boruta"`: Wrapper method that iteratively removes unimportant features using random forest.

  - `"roc"`: Selects features based on Receiver Operating Characteristic (ROC) AUC scores.

  - `"infgain"`: Uses Information Gain to select the most informative features.

  - `"mrmr"`: Minimum Redundancy Maximum Relevance (mRMR), selects features that maximize relevance and minimize redundancy.

  - `"corr"`: Filters features based on correlation thresholds to reduce multicollinearity.

`models` and `selector.recipes` will be paired in the order they are provided. If the number of recipes
does not match the number of models, the first recipe will be used for all models.}

\item{tuning.method}{A character string specifying the hyperparameter tuning strategy. Options include:
- `"tune_grid"` (default): Grid search across a pre-defined set of hyperparameters.
- `"tune_race_anova"`: Adaptive search using ANOVA-based pruning to speed up tuning.
- `"tune_race_win_loss"`: Win-loss-based adaptive search that discards weak candidates early.
- `"tune_bayes"`: Bayesian optimization to intelligently explore hyperparameter space.
- `"tune_sim_anneal"`: Simulated annealing, a probabilistic approach to global optimization.}

\item{n}{An integer specifying the number of iterations for the tuning method.}

\item{v}{An integer specifying the number of folds for the cross-validation during the hyperparameters tuning.}

\item{boruta.maxRuns}{maxRuns parameter to pass to boruta feature selector when it is selected.}

\item{selector.threshold}{Threshold parameter used for feature selection (Applied on "roc", "infgain", "mrmr", "corr" selectors).}

\item{metric}{A character string specifying the metric to be used for tuning.}

\item{nsim}{An integer specifying the number of simulations for the permutation-based VIP.}

\item{filter}{A logical specifying whether to filter genes not annotated in the GO and KEGG databases.}

\item{seed}{An integer specifying the seed for reproducibility.}

\item{downsample}{A logical specifying whether to downsample the majority class.}

\item{plot}{A logical specifying whether to generate plots. Default is TRUE.}

\item{parallel}{A logical specifying whether to use parallel computation for model fitting and variable importance. Default is FALSE.

  - TRUE: Use all available cores for parallel computation.

  - FALSE: Run all computations sequentially.

  This affects both model tuning and the permutation-based variable importance calculation.}
}
\value{
An object of class `runClassifiers.obj` containing:
  - `data`: A list containing the adjusted data and metadata.
  - `models.info`: A list containing the finalized workflows for each model.
  - `model.features`: A list containing the variable importance for each model.
  - `performances`: A list containing the tuning and final metrics for each model.
  - `predictions`: A data frame containing the predictions for each model.
}
\description{
This function run classifiers specified on an object of class preProcess.obj or
on the data provided in the arguments ...
}
\details{
The function runs specified classifiers paired with provided selector.recipes on pre-processed data.
If filter = TRUE, the function first filters the genes that are not annotated in the GO and KEGG databases.
Then, the function tunes and fits the models using the specified tuning method.
Finally it computes the variable importances for each model using the permutation-based VIP,
when the direct VIP computation fails.
}
\examples{
\dontrun{
rc <- runClassifiers(preProcess.obj,
  models = c("bag_mlp", "rand_forest", "svm_poly"),
  selector.recipes = "boruta", tuning.method = "tune_grid", n = 5,
  v = 3, metric = "accuracy", nsim = 2, seed = 123
)
}

}
